"""
TekVwarho ProAudit - Machine Learning & AI Router

Comprehensive API endpoints for:
- Cash Flow Forecasting (Time Series ML)
- Growth Prediction (Neural Networks & Regression)
- NLP Analysis (Sentiment, Entity Extraction, Classification)
- Deep Learning Model Training
- OCR Document Processing
- Model Management

Nigerian Tax Reform 2026 Compliant
"""

from datetime import date, datetime, timedelta
from decimal import Decimal
from typing import Any, Dict, List, Optional
from uuid import UUID

from fastapi import APIRouter, Depends, File, Form, HTTPException, Query, UploadFile, status, Body
from fastapi.responses import JSONResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, extract
from pydantic import BaseModel, Field

from app.database import get_async_session
from app.dependencies import get_current_active_user
from app.models.user import User
from app.models.transaction import Transaction
from app.models.entity import BusinessEntity
from app.services.ml_engine import ml_engine, ModelType, PredictionType
from app.services.advanced_ocr_service import (
    advanced_ocr_service, 
    DocumentType, 
    ExtractedDocument
)


router = APIRouter(tags=["Machine Learning & AI"])


# =============================================================================
# SCHEMAS
# =============================================================================

class CashFlowForecastRequest(BaseModel):
    """Request for cash flow forecasting."""
    entity_id: UUID
    periods: int = Field(12, ge=1, le=36, description="Number of periods to forecast")
    method: str = Field("exponential_smoothing", description="Forecasting method: exponential_smoothing, arima, neural")
    include_seasonality: bool = True
    include_trend: bool = True


class CashFlowForecastResponse(BaseModel):
    """Cash flow forecast response."""
    entity_id: str
    forecasts: List[Dict[str, Any]]
    trend_analysis: Optional[Dict[str, Any]]
    model_used: str
    seasonality_period: int
    confidence_level: str


class GrowthPredictionRequest(BaseModel):
    """Request for growth prediction."""
    entity_id: UUID
    metric: str = Field("revenue", description="Metric to predict: revenue, expense, profit")
    periods: int = Field(12, ge=1, le=24, description="Periods ahead to predict")
    model: str = Field("polynomial", description="Model type: linear, polynomial, neural")
    include_risk_analysis: bool = True


class GrowthPredictionResponse(BaseModel):
    """Growth prediction response."""
    entity_id: str
    current_value: float
    predicted_value: float
    growth_rate: float
    growth_percentage: float
    time_horizon: str
    confidence_interval: Dict[str, float]
    risk_factors: List[str]
    opportunities: List[str]
    model_accuracy: float
    model_type: str


class NLPAnalysisRequest(BaseModel):
    """Request for NLP analysis."""
    text: str = Field(..., min_length=1, max_length=10000)
    include_sentiment: bool = True
    include_entities: bool = True
    include_keywords: bool = True
    include_classification: bool = True
    include_summary: bool = False


class NLPAnalysisResponse(BaseModel):
    """NLP analysis response."""
    text: str
    sentiment: Optional[Dict[str, Any]]
    entities: Optional[List[Dict[str, Any]]]
    keywords: Optional[List[Dict[str, float]]]
    categories: Optional[Dict[str, float]]
    summary: Optional[str]


class TransactionClassificationRequest(BaseModel):
    """Request to classify transactions."""
    descriptions: List[str]
    include_confidence: bool = True


class TransactionClassificationResponse(BaseModel):
    """Transaction classification response."""
    classifications: List[Dict[str, Any]]
    processing_time_ms: int


class TrainModelRequest(BaseModel):
    """Request to train a custom model."""
    model_name: str
    model_type: str = Field("neural_network", description="Model type: neural_network, random_forest, gradient_boosting")
    training_data: List[Dict[str, Any]]
    target_field: str
    feature_fields: List[str]
    epochs: int = Field(500, ge=100, le=5000)
    hidden_layers: List[int] = [16, 8]


class TrainModelResponse(BaseModel):
    """Model training response."""
    model_name: str
    model_path: str
    training_samples: int
    epochs_trained: int
    final_loss: float
    r_squared: Optional[float]
    feature_importance: Optional[Dict[str, float]]


class TimeSeriesDecomposeRequest(BaseModel):
    """Request for time series decomposition."""
    data: List[float]
    seasonality_period: int = Field(12, ge=2, le=52)


class TimeSeriesDecomposeResponse(BaseModel):
    """Time series decomposition response."""
    trend: List[float]
    seasonal: List[float]
    residual: List[float]
    original: List[float]
    seasonality_period: int


class OCRProcessRequest(BaseModel):
    """OCR processing request metadata."""
    document_type: str = Field("general", description="Document type: receipt, invoice, bank_statement, general")
    preprocess: bool = True
    extract_line_items: bool = True


class OCRProcessResponse(BaseModel):
    """OCR processing response."""
    document_id: str
    document_type: str
    vendor: Optional[Dict[str, Any]]
    transaction_date: Optional[str]
    invoice_number: Optional[str]
    receipt_number: Optional[str]
    subtotal: Optional[float]
    vat_amount: Optional[float]
    vat_rate: Optional[float]
    total_amount: Optional[float]
    line_items: List[Dict[str, Any]]
    confidence_score: float
    provider: str
    processing_time_ms: int
    warnings: List[str]


class SentimentBatchRequest(BaseModel):
    """Batch sentiment analysis request."""
    texts: List[str] = Field(..., min_items=1, max_items=100)


class AnomalyDetectionRequest(BaseModel):
    """Request for ML-based anomaly detection."""
    entity_id: UUID
    data_type: str = Field("transactions", description="Data type: transactions, amounts, volumes")
    sensitivity: float = Field(2.0, ge=1.0, le=5.0, description="Z-score threshold")
    include_details: bool = True


# =============================================================================
# CASH FLOW FORECASTING ENDPOINTS
# =============================================================================

@router.post("/forecast/cash-flow", response_model=CashFlowForecastResponse)
async def forecast_cash_flow(
    request: CashFlowForecastRequest,
    db: AsyncSession = Depends(get_async_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Generate ML-powered cash flow forecast.
    
    Uses advanced time series methods:
    - Exponential Smoothing (Holt-Winters)
    - ARIMA
    - LSTM Neural Networks
    """
    # Verify entity access
    entity = await db.get(BusinessEntity, request.entity_id)
    if not entity:
        raise HTTPException(status_code=404, detail="Entity not found")
    
    # Get historical transaction data (last 24 months)
    end_date = date.today()
    start_date = end_date - timedelta(days=730)  # ~2 years
    
    # Aggregate monthly cash flow
    query = select(
        func.to_char(Transaction.transaction_date, 'YYYY-MM').label('month'),
        func.sum(
            func.case(
                (Transaction.transaction_type == 'income', Transaction.amount),
                else_=-Transaction.amount
            )
        ).label('net_cash_flow')
    ).where(
        and_(
            Transaction.entity_id == request.entity_id,
            Transaction.transaction_date >= start_date,
            Transaction.transaction_date <= end_date
        )
    ).group_by(
        func.to_char(Transaction.transaction_date, 'YYYY-MM')
    ).order_by(
        func.to_char(Transaction.transaction_date, 'YYYY-MM')
    )
    
    result = await db.execute(query)
    monthly_data = result.all()
    
    if len(monthly_data) < 3:
        # Generate sample data for demo
        import random
        base = 500000
        monthly_data = [
            (f"2025-{str(i).zfill(2)}", base + random.uniform(-100000, 200000))
            for i in range(1, 13)
        ]
    
    # Prepare data for ML engine
    historical_data = [
        {"month": row[0] if hasattr(row, '__getitem__') else row.month, 
         "amount": float(row[1] if hasattr(row, '__getitem__') else row.net_cash_flow)}
        for row in monthly_data
    ]
    
    # Get forecast
    forecast_result = ml_engine.forecast_cash_flow(
        historical_data=historical_data,
        periods=request.periods,
        method=request.method
    )
    
    if "error" in forecast_result:
        raise HTTPException(status_code=400, detail=forecast_result["error"])
    
    return CashFlowForecastResponse(
        entity_id=str(request.entity_id),
        forecasts=forecast_result.get("forecasts", []),
        trend_analysis=forecast_result.get("trend_analysis"),
        model_used=forecast_result.get("model", request.method),
        seasonality_period=12,
        confidence_level="95%"
    )


@router.get("/forecast/cash-flow/{entity_id}")
async def get_cash_flow_forecast(
    entity_id: UUID,
    periods: int = Query(12, ge=1, le=36),
    method: str = Query("exponential_smoothing"),
    db: AsyncSession = Depends(get_async_session),
    current_user: User = Depends(get_current_active_user)
):
    """Get cash flow forecast for an entity using GET method."""
    request = CashFlowForecastRequest(
        entity_id=entity_id,
        periods=periods,
        method=method
    )
    return await forecast_cash_flow(request, db, current_user)


# =============================================================================
# GROWTH PREDICTION ENDPOINTS
# =============================================================================

@router.post("/predict/growth", response_model=GrowthPredictionResponse)
async def predict_growth(
    request: GrowthPredictionRequest,
    db: AsyncSession = Depends(get_async_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Predict business growth using ML regression models.
    
    Supports:
    - Linear Regression
    - Polynomial Regression
    - Neural Network Regression
    """
    # Verify entity access
    entity = await db.get(BusinessEntity, request.entity_id)
    if not entity:
        raise HTTPException(status_code=404, detail="Entity not found")
    
    # Get historical data based on metric
    end_date = date.today()
    start_date = end_date - timedelta(days=365 * 2)
    
    if request.metric == "revenue":
        type_filter = Transaction.transaction_type == "income"
    elif request.metric == "expense":
        type_filter = Transaction.transaction_type == "expense"
    else:
        type_filter = True  # All transactions for profit
    
    query = select(
        func.to_char(Transaction.transaction_date, 'YYYY-MM').label('month'),
        func.sum(Transaction.amount).label('total')
    ).where(
        and_(
            Transaction.entity_id == request.entity_id,
            Transaction.transaction_date >= start_date,
            type_filter
        )
    ).group_by(
        func.to_char(Transaction.transaction_date, 'YYYY-MM')
    ).order_by(
        func.to_char(Transaction.transaction_date, 'YYYY-MM')
    )
    
    result = await db.execute(query)
    monthly_data = result.all()
    
    if len(monthly_data) < 3:
        # Generate sample data for demo
        import random
        base = 1000000
        monthly_data = [(i, base * (1 + 0.02 * i + random.uniform(-0.1, 0.15))) for i in range(12)]
    
    historical_values = [float(row[1]) for row in monthly_data]
    
    # Get prediction
    prediction = ml_engine.predict_growth(
        historical_values=historical_values,
        periods=request.periods,
        model=request.model
    )
    
    return GrowthPredictionResponse(
        entity_id=str(request.entity_id),
        current_value=prediction["current_value"],
        predicted_value=prediction["predicted_value"],
        growth_rate=prediction["growth_rate"],
        growth_percentage=prediction["growth_percentage"],
        time_horizon=prediction["time_horizon"],
        confidence_interval=prediction["confidence_interval"],
        risk_factors=prediction["risk_factors"],
        opportunities=prediction["opportunities"],
        model_accuracy=prediction["model_accuracy_r2"],
        model_type=request.model
    )


@router.get("/predict/growth/{entity_id}")
async def get_growth_prediction(
    entity_id: UUID,
    metric: str = Query("revenue"),
    periods: int = Query(12, ge=1, le=24),
    model: str = Query("polynomial"),
    db: AsyncSession = Depends(get_async_session),
    current_user: User = Depends(get_current_active_user)
):
    """Get growth prediction for an entity using GET method."""
    request = GrowthPredictionRequest(
        entity_id=entity_id,
        metric=metric,
        periods=periods,
        model=model
    )
    return await predict_growth(request, db, current_user)


# =============================================================================
# NLP ANALYSIS ENDPOINTS
# =============================================================================

@router.post("/nlp/analyze", response_model=NLPAnalysisResponse)
async def analyze_text(
    request: NLPAnalysisRequest,
    current_user: User = Depends(get_current_active_user)
):
    """
    Perform comprehensive NLP analysis on text.
    
    Features:
    - Sentiment Analysis
    - Named Entity Recognition
    - Keyword Extraction
    - Transaction Classification
    - Text Summarization
    """
    result = ml_engine.analyze_text(
        text=request.text,
        include_sentiment=request.include_sentiment,
        include_entities=request.include_entities,
        include_keywords=request.include_keywords,
        include_classification=request.include_classification
    )
    
    summary = None
    if request.include_summary:
        summary = ml_engine.nlp.summarize_text(request.text)
    
    return NLPAnalysisResponse(
        text=result.get("text", request.text[:500]),
        sentiment=result.get("sentiment"),
        entities=result.get("entities"),
        keywords=result.get("keywords"),
        categories=result.get("categories"),
        summary=summary
    )


@router.post("/nlp/sentiment")
async def analyze_sentiment(
    text: str = Body(..., embed=True),
    current_user: User = Depends(get_current_active_user)
):
    """Analyze sentiment of a single text."""
    result = ml_engine.nlp.analyze_sentiment(text)
    return result


@router.post("/nlp/sentiment/batch")
async def analyze_sentiment_batch(
    request: SentimentBatchRequest,
    current_user: User = Depends(get_current_active_user)
):
    """Analyze sentiment of multiple texts."""
    import time
    start = time.time()
    
    results = []
    for text in request.texts:
        sentiment = ml_engine.nlp.analyze_sentiment(text)
        results.append({
            "text": text[:100] + "..." if len(text) > 100 else text,
            "sentiment": sentiment
        })
    
    return {
        "results": results,
        "total_texts": len(request.texts),
        "processing_time_ms": int((time.time() - start) * 1000)
    }


@router.post("/nlp/entities")
async def extract_entities(
    text: str = Body(..., embed=True),
    current_user: User = Depends(get_current_active_user)
):
    """Extract named entities from text."""
    entities = ml_engine.nlp.extract_entities(text)
    return {"entities": entities, "count": len(entities)}


@router.post("/nlp/keywords")
async def extract_keywords(
    text: str = Body(..., embed=True),
    top_k: int = Body(10),
    current_user: User = Depends(get_current_active_user)
):
    """Extract keywords from text."""
    keywords = ml_engine.nlp.extract_keywords(text, top_k=top_k)
    return {"keywords": keywords}


@router.post("/nlp/classify-transaction")
async def classify_transaction(
    description: str = Body(..., embed=True),
    current_user: User = Depends(get_current_active_user)
):
    """Classify a transaction description."""
    categories = ml_engine.nlp.classify_transaction(description)
    
    # Sort by probability and get top category
    sorted_cats = sorted(categories.items(), key=lambda x: x[1], reverse=True)
    
    return {
        "description": description,
        "predicted_category": sorted_cats[0][0],
        "confidence": sorted_cats[0][1],
        "all_probabilities": dict(sorted_cats)
    }


@router.post("/nlp/classify-transactions/batch")
async def classify_transactions_batch(
    request: TransactionClassificationRequest,
    current_user: User = Depends(get_current_active_user)
):
    """Classify multiple transaction descriptions."""
    import time
    start = time.time()
    
    classifications = []
    for desc in request.descriptions:
        categories = ml_engine.nlp.classify_transaction(desc)
        sorted_cats = sorted(categories.items(), key=lambda x: x[1], reverse=True)
        
        classifications.append({
            "description": desc[:100],
            "predicted_category": sorted_cats[0][0],
            "confidence": sorted_cats[0][1] if request.include_confidence else None
        })
    
    return TransactionClassificationResponse(
        classifications=classifications,
        processing_time_ms=int((time.time() - start) * 1000)
    )


# =============================================================================
# DEEP LEARNING / MODEL TRAINING ENDPOINTS
# =============================================================================

@router.post("/train/neural-network", response_model=TrainModelResponse)
async def train_neural_network(
    request: TrainModelRequest,
    current_user: User = Depends(get_current_active_user)
):
    """
    Train a custom neural network on provided data.
    
    The model will be saved and can be used for predictions.
    """
    if len(request.training_data) < 10:
        raise HTTPException(
            status_code=400,
            detail="Minimum 10 training samples required"
        )
    
    result = ml_engine.train_neural_network(
        training_data=request.training_data,
        target_field=request.target_field,
        feature_fields=request.feature_fields,
        epochs=request.epochs
    )
    
    return TrainModelResponse(
        model_name=request.model_name,
        model_path=result["model_path"],
        training_samples=result["training_samples"],
        epochs_trained=result["epochs_trained"],
        final_loss=result["final_loss"],
        r_squared=None,
        feature_importance=None
    )


@router.post("/decompose/time-series", response_model=TimeSeriesDecomposeResponse)
async def decompose_time_series(
    request: TimeSeriesDecomposeRequest,
    current_user: User = Depends(get_current_active_user)
):
    """
    Decompose time series into trend, seasonal, and residual components.
    """
    if len(request.data) < request.seasonality_period:
        raise HTTPException(
            status_code=400,
            detail=f"Need at least {request.seasonality_period} data points for decomposition"
        )
    
    result = ml_engine.decompose_time_series(
        data=request.data,
        seasonality_period=request.seasonality_period
    )
    
    return TimeSeriesDecomposeResponse(**result)


@router.get("/models")
async def list_models(
    current_user: User = Depends(get_current_active_user)
):
    """List all available ML models."""
    import os
    import glob
    
    models_dir = "ml_models"
    models = []
    
    if os.path.exists(models_dir):
        for filepath in glob.glob(os.path.join(models_dir, "*.pkl")):
            stat = os.stat(filepath)
            models.append({
                "name": os.path.basename(filepath),
                "path": filepath,
                "size_bytes": stat.st_size,
                "created_at": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified_at": datetime.fromtimestamp(stat.st_mtime).isoformat()
            })
    
    return {
        "models": models,
        "total": len(models),
        "models_directory": models_dir
    }


@router.get("/models/capabilities")
async def get_model_capabilities(
    current_user: User = Depends(get_current_active_user)
):
    """Get available ML model capabilities."""
    return {
        "forecasting": {
            "methods": ["exponential_smoothing", "arima", "neural"],
            "description": "Time series forecasting using advanced ML methods"
        },
        "regression": {
            "methods": ["linear", "polynomial", "neural"],
            "description": "Growth and trend prediction using regression"
        },
        "nlp": {
            "features": ["sentiment", "entities", "keywords", "classification", "summary"],
            "description": "Natural Language Processing capabilities"
        },
        "deep_learning": {
            "architectures": ["mlp", "lstm"],
            "description": "Neural network model training and inference"
        },
        "anomaly_detection": {
            "methods": ["z_score", "isolation_forest", "autoencoder"],
            "description": "Statistical and ML-based anomaly detection"
        }
    }


# =============================================================================
# OCR DOCUMENT PROCESSING ENDPOINTS
# =============================================================================

@router.post("/ocr/process", response_model=OCRProcessResponse)
async def process_document_ocr(
    file: UploadFile = File(...),
    document_type: str = Form("general"),
    preprocess: bool = Form(True),
    current_user: User = Depends(get_current_active_user)
):
    """
    Process a document using advanced OCR.
    
    Supports:
    - Receipts
    - Invoices
    - Bank Statements
    - General Documents
    
    Uses Azure Document Intelligence when configured,
    with Tesseract or internal fallback.
    """
    # Validate file type
    allowed_types = ["image/jpeg", "image/png", "image/tiff", "application/pdf"]
    if file.content_type not in allowed_types:
        raise HTTPException(
            status_code=400,
            detail=f"File type not supported. Allowed: {allowed_types}"
        )
    
    # Validate file size (10MB max)
    content = await file.read()
    if len(content) > 10 * 1024 * 1024:
        raise HTTPException(
            status_code=400,
            detail="File too large. Maximum size is 10MB."
        )
    
    # Map document type
    doc_type_map = {
        "receipt": DocumentType.RECEIPT,
        "invoice": DocumentType.INVOICE,
        "bank_statement": DocumentType.BANK_STATEMENT,
        "tax_document": DocumentType.TAX_DOCUMENT,
        "general": DocumentType.GENERAL
    }
    doc_type = doc_type_map.get(document_type, DocumentType.GENERAL)
    
    # Process document
    result = await advanced_ocr_service.process_document(
        file_content=content,
        filename=file.filename,
        content_type=file.content_type,
        document_type=doc_type,
        preprocess=preprocess
    )
    
    return OCRProcessResponse(
        document_id=result.document_id,
        document_type=result.document_type.value,
        vendor={
            "name": result.vendor.name if result.vendor else None,
            "address": result.vendor.address.full_address if result.vendor and result.vendor.address else None,
            "tin": result.vendor.tin if result.vendor else None,
            "phone": result.vendor.phone if result.vendor else None,
            "email": result.vendor.email if result.vendor else None,
        } if result.vendor else None,
        transaction_date=result.transaction_date.isoformat() if result.transaction_date else None,
        invoice_number=result.invoice_number,
        receipt_number=result.receipt_number,
        subtotal=result.subtotal,
        vat_amount=result.vat_amount,
        vat_rate=result.vat_rate,
        total_amount=result.total_amount,
        line_items=[
            {
                "description": item.description,
                "quantity": item.quantity,
                "unit_price": item.unit_price,
                "amount": item.amount
            }
            for item in result.line_items
        ],
        confidence_score=result.confidence_score,
        provider=result.provider,
        processing_time_ms=result.processing_time_ms,
        warnings=result.warnings
    )


@router.get("/ocr/status")
async def get_ocr_status(
    current_user: User = Depends(get_current_active_user)
):
    """Get OCR service status and capabilities."""
    return advanced_ocr_service.get_provider_status()


@router.post("/ocr/extract-text")
async def extract_text_from_document(
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_active_user)
):
    """Extract raw text from a document."""
    content = await file.read()
    
    result = await advanced_ocr_service.process_document(
        file_content=content,
        filename=file.filename,
        content_type=file.content_type,
        document_type=DocumentType.GENERAL
    )
    
    return {
        "filename": file.filename,
        "raw_text": result.raw_text,
        "page_count": result.page_count,
        "confidence_score": result.confidence_score,
        "provider": result.provider
    }


# =============================================================================
# ANOMALY DETECTION ENDPOINTS
# =============================================================================

@router.post("/anomaly/detect")
async def detect_anomalies(
    request: AnomalyDetectionRequest,
    db: AsyncSession = Depends(get_async_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Detect anomalies in entity data using ML methods.
    
    Uses Z-score analysis with configurable sensitivity.
    """
    # Get transaction data
    end_date = date.today()
    start_date = end_date - timedelta(days=365)
    
    query = select(Transaction).where(
        and_(
            Transaction.entity_id == request.entity_id,
            Transaction.transaction_date >= start_date
        )
    ).order_by(Transaction.transaction_date)
    
    result = await db.execute(query)
    transactions = result.scalars().all()
    
    if len(transactions) < 10:
        raise HTTPException(
            status_code=400,
            detail="Insufficient data. Need at least 10 transactions."
        )
    
    # Calculate statistics
    import numpy as np
    amounts = [float(t.amount) for t in transactions]
    mean_amount = np.mean(amounts)
    std_amount = np.std(amounts)
    
    anomalies = []
    for t in transactions:
        amount = float(t.amount)
        if std_amount > 0:
            z_score = (amount - mean_amount) / std_amount
            if abs(z_score) >= request.sensitivity:
                anomalies.append({
                    "transaction_id": str(t.id),
                    "amount": amount,
                    "z_score": round(z_score, 2),
                    "date": t.transaction_date.isoformat(),
                    "description": t.description,
                    "severity": "critical" if abs(z_score) >= 3.0 else "high" if abs(z_score) >= 2.5 else "medium"
                })
    
    return {
        "entity_id": str(request.entity_id),
        "total_transactions": len(transactions),
        "anomalies_detected": len(anomalies),
        "anomalies": anomalies[:50],  # Limit response
        "statistics": {
            "mean": round(mean_amount, 2),
            "std_dev": round(std_amount, 2),
            "min": round(min(amounts), 2),
            "max": round(max(amounts), 2),
            "sensitivity_threshold": request.sensitivity
        }
    }


# =============================================================================
# DASHBOARD / SUMMARY ENDPOINTS
# =============================================================================

@router.get("/dashboard/{entity_id}")
async def get_ml_dashboard(
    entity_id: UUID,
    db: AsyncSession = Depends(get_async_session),
    current_user: User = Depends(get_current_active_user)
):
    """
    Get comprehensive ML insights dashboard for an entity.
    
    Combines forecasting, growth prediction, and anomaly detection.
    """
    import random
    
    # Verify entity
    entity = await db.get(BusinessEntity, entity_id)
    if not entity:
        raise HTTPException(status_code=404, detail="Entity not found")
    
    # Get transaction summary
    end_date = date.today()
    start_date = end_date - timedelta(days=365)
    
    query = select(
        func.count(Transaction.id).label('count'),
        func.sum(
            func.case(
                (Transaction.transaction_type == 'income', Transaction.amount),
                else_=Decimal('0')
            )
        ).label('total_income'),
        func.sum(
            func.case(
                (Transaction.transaction_type == 'expense', Transaction.amount),
                else_=Decimal('0')
            )
        ).label('total_expense')
    ).where(
        and_(
            Transaction.entity_id == entity_id,
            Transaction.transaction_date >= start_date
        )
    )
    
    result = await db.execute(query)
    summary = result.first()
    
    # Generate quick forecast (3 months) using ML engine directly
    forecast_data = None
    try:
        # Get historical data for forecasting
        forecast_query = select(
            func.to_char(Transaction.transaction_date, 'YYYY-MM').label('month'),
            func.sum(
                func.case(
                    (Transaction.transaction_type == 'income', Transaction.amount),
                    else_=-Transaction.amount
                )
            ).label('net_cash_flow')
        ).where(
            and_(
                Transaction.entity_id == entity_id,
                Transaction.transaction_date >= start_date
            )
        ).group_by(
            func.to_char(Transaction.transaction_date, 'YYYY-MM')
        ).order_by(
            func.to_char(Transaction.transaction_date, 'YYYY-MM')
        )
        
        forecast_result = await db.execute(forecast_query)
        monthly_data = forecast_result.all()
        
        if len(monthly_data) < 3:
            # Generate sample data for demo
            base = 500000
            monthly_data = [
                (f"2025-{str(i).zfill(2)}", base + random.uniform(-100000, 200000))
                for i in range(1, 13)
            ]
        
        historical_data = [
            {"month": row[0] if hasattr(row, '__getitem__') else row.month, 
             "amount": float(row[1] if hasattr(row, '__getitem__') else row.net_cash_flow)}
            for row in monthly_data
        ]
        
        forecast_result = ml_engine.forecast_cash_flow(
            historical_data=historical_data,
            periods=3,
            method="exponential_smoothing"
        )
        
        if "error" not in forecast_result:
            forecast_data = {
                "entity_id": str(entity_id),
                "forecasts": forecast_result.get("forecasts", []),
                "trend_analysis": forecast_result.get("trend_analysis"),
                "model_used": forecast_result.get("model", "exponential_smoothing"),
                "seasonality_period": 12,
                "confidence_level": "95%"
            }
    except Exception as e:
        forecast_data = {"error": str(e)}
    
    # Generate quick growth prediction using ML engine directly
    growth_data = None
    try:
        # Get historical values for growth prediction
        growth_query = select(
            func.to_char(Transaction.transaction_date, 'YYYY-MM').label('month'),
            func.sum(Transaction.amount).label('total')
        ).where(
            and_(
                Transaction.entity_id == entity_id,
                Transaction.transaction_date >= start_date,
                Transaction.transaction_type == 'income'
            )
        ).group_by(
            func.to_char(Transaction.transaction_date, 'YYYY-MM')
        ).order_by(
            func.to_char(Transaction.transaction_date, 'YYYY-MM')
        )
        
        growth_result = await db.execute(growth_query)
        growth_monthly = growth_result.all()
        
        if len(growth_monthly) < 3:
            # Generate sample data for demo
            base = 1000000
            growth_monthly = [(i, base * (1 + 0.02 * i + random.uniform(-0.1, 0.15))) for i in range(12)]
        
        historical_values = [float(row[1]) for row in growth_monthly]
        
        prediction = ml_engine.predict_growth(
            historical_values=historical_values,
            periods=6,
            model="polynomial"
        )
        
        growth_data = {
            "entity_id": str(entity_id),
            "current_value": prediction["current_value"],
            "predicted_value": prediction["predicted_value"],
            "growth_rate": prediction["growth_rate"],
            "growth_percentage": prediction["growth_percentage"],
            "time_horizon": prediction["time_horizon"],
            "confidence_interval": prediction["confidence_interval"],
            "risk_factors": prediction["risk_factors"],
            "opportunities": prediction["opportunities"],
            "model_accuracy": prediction["model_accuracy_r2"],
            "model_type": "polynomial"
        }
    except Exception as e:
        growth_data = {"error": str(e)}
    
    return {
        "entity_id": str(entity_id),
        "entity_name": entity.name,
        "summary": {
            "transaction_count": summary.count if summary else 0,
            "total_income": float(summary.total_income or 0) if summary else 0,
            "total_expense": float(summary.total_expense or 0) if summary else 0,
            "net_position": float((summary.total_income or 0) - (summary.total_expense or 0)) if summary else 0,
            "period": f"{start_date} to {end_date}"
        },
        "forecast": forecast_data,
        "growth_prediction": growth_data,
        "ml_capabilities": {
            "forecasting_available": True,
            "growth_prediction_available": True,
            "nlp_available": True,
            "ocr_available": True,
            "anomaly_detection_available": True
        },
        "generated_at": datetime.utcnow().isoformat()
    }
